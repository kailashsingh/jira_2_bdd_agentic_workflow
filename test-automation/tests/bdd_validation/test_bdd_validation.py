"""
BDD Validation Tests - Core functionality for testing backend BDD generation
Tests that our backend generates correct BDD scenarios from JIRA requirements
"""

import pytest
import asyncio
from typing import Dict, List, Any
from unittest.mock import Mock, patch, AsyncMock

from src.models.model_manager import ModelManager, BDDValidationResult
from src.utils.test_helpers import TestDataGenerator
from src.backend_integration import BackendTestClient, AgentTestRunner
from src.config.logging import get_logger

logger = get_logger(__name__)

@pytest.mark.bdd
@pytest.mark.integration
class TestBDDValidation:
    """Test BDD scenario generation and validation against backend results."""
    
    @pytest.fixture
    def model_manager(self):
        """Initialize model manager for BDD validation."""
        return ModelManager()
    
    @pytest.fixture
    def test_data_generator(self):
        """Initialize test data generator."""
        return TestDataGenerator()
    
    @pytest.fixture
    async def backend_client(self):
        """Setup backend client for testing."""
        client = BackendTestClient()
        await client.initialize_async_client()
        yield client
        await client.close_async_client()

    def test_bdd_validation_structure_check(self, model_manager, test_data_generator):
        """Test that BDD validation correctly identifies structure elements."""
        
        # Generate test ticket
        jira_ticket = test_data_generator.generate_jira_ticket(
            summary="User login functionality",
            description="Implement secure user authentication with email and password",
            issue_type="Story"
        )
        
        # Mock backend result (what our backend actually generates)
        backend_result = {
            "feature": """@TEST-123 @AutoGenerated
Feature: User authentication
  As a user
  I want to login to the system
  So that I can access my account

  Scenario: Successful login with valid credentials
    Given user is on login page
    When user enters valid email and password
    Then user should be redirected to dashboard
    And user session should be created
""",
            "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';

Given('user is on login page', async () => {
    await browser.url('/login');
});

When('user enters valid email and password', async () => {
    await $('#email').setValue('test@example.com');
    await $('#password').setValue('password123');
    await $('button=Login').click();
});

Then('user should be redirected to dashboard', async () => {
    await expect(browser).toHaveUrl('/dashboard');
});
""",
            "ticket_key": "TEST-123"
        }
        
        # Expected result (what we expect to be generated)
        expected_result = {
            "feature": """@TEST-123 @AutoGenerated
Feature: User authentication
  As a user
  I want to login to the system
  So that I can access my account

  Scenario: Successful login with valid credentials
    Given user is on login page
    When user enters valid credentials
    Then user should be redirected to dashboard
""",
            "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';

Given('user is on login page', async () => {
    await browser.url('/login');
});

When('user enters valid credentials', async () => {
    await $('#email').setValue('test@example.com');
    await $('#password').setValue('password123');
    await $('button=Login').click();
});

Then('user should be redirected to dashboard', async () => {
    await expect(browser).toHaveUrl('/dashboard');
});
"""
        }
        
        # Validate backend result against expected result
        validation_result = model_manager.validate_backend_bdd_results(
            backend_result, expected_result, jira_ticket
        )
        
        # Assertions
        assert isinstance(validation_result, BDDValidationResult)
        assert validation_result.structure_match is True, "BDD structure should be valid"
        assert validation_result.scenario_count_match is True, "Scenario count should match"
        assert validation_result.similarity_score > 0.7, f"Similarity too low: {validation_result.similarity_score}"
        assert validation_result.validation_passed is True, f"Validation failed: {validation_result.differences}"
        
        logger.info(f"✅ BDD validation passed with similarity: {validation_result.similarity_score:.2f}")

    def test_bdd_validation_failure_cases(self, model_manager, test_data_generator):
        """Test that BDD validation correctly identifies failures."""
        
        jira_ticket = test_data_generator.generate_jira_ticket()
        
        # Backend result with missing structure elements
        invalid_backend_result = {
            "feature": "Invalid feature without proper Gherkin syntax",
            "step_definitions": "No proper step definitions",
            "ticket_key": "TEST-123"
        }
        
        # Valid expected result
        expected_result = {
            "feature": """@TEST-123 @AutoGenerated
Feature: Valid feature
  Scenario: Valid scenario
    Given valid step
    When valid action
    Then valid result
""",
            "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';"""
        }
        
        # Validate - should fail
        validation_result = model_manager.validate_backend_bdd_results(
            invalid_backend_result, expected_result, jira_ticket
        )
        
        # Assertions for failure
        assert validation_result.validation_passed is False, "Validation should fail for invalid BDD"
        assert validation_result.structure_match is False, "Structure should not match"
        assert len(validation_result.differences) > 0, "Should identify differences"
        
        logger.info(f"✅ BDD validation correctly identified failures: {len(validation_result.differences)} issues")

    @pytest.mark.asyncio
    async def test_backend_bdd_generation_validation(self, model_manager, backend_client, test_data_generator):
        """Test end-to-end BDD generation and validation with backend."""
        
        # Generate test ticket
        test_ticket = test_data_generator.generate_jira_ticket(
            summary="Shopping cart checkout process",
            description="Implement checkout functionality for e-commerce cart",
            issue_type="Story"
        )
        
        # Mock backend BDD generation
        with patch('backend.src.agents.orchestrator.WorkflowOrchestrator') as mock_orchestrator:
            mock_instance = mock_orchestrator.return_value
            
            # Mock backend response
            mock_backend_result = {
                "feature": f"""@{test_ticket['key']} @AutoGenerated
Feature: Shopping cart checkout
  As a customer
  I want to checkout my cart
  So that I can purchase items

  Scenario: Successful checkout with valid payment
    Given user has items in cart
    When user proceeds to checkout
    And user enters valid payment details
    Then order should be created
    And payment should be processed
""",
                "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';

Given('user has items in cart', async () => {
    await browser.url('/cart');
    await expect($('.cart-item')).toBeExisting();
});

When('user proceeds to checkout', async () => {
    await $('button=Checkout').click();
});

When('user enters valid payment details', async () => {
    await $('#card-number').setValue('4111111111111111');
    await $('#expiry').setValue('12/25');
    await $('#cvv').setValue('123');
    await $('button=Pay').click();
});

Then('order should be created', async () => {
    await expect($('.order-confirmation')).toBeDisplayed();
});
""",
                "ticket_key": test_ticket['key']
            }
            
            mock_instance.process_tickets = AsyncMock(return_value={
                'status': 'completed',
                'result': {'generated_tests': mock_backend_result}
            })
            
            # Trigger backend workflow
            workflow_result = await backend_client.trigger_workflow_test([test_ticket['key']])
            
            # Validate workflow execution
            assert workflow_result.success, f"Workflow failed: {workflow_result.error_message}"
            
            # Expected result for validation
            expected_result = {
                "feature": f"""@{test_ticket['key']} @AutoGenerated
Feature: Shopping cart checkout
  As a customer
  I want to checkout my cart
  So that I can purchase items

  Scenario: Successful checkout with valid payment
    Given user has items in cart
    When user proceeds to checkout
    And user enters payment details
    Then order should be created
""",
                "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';"""
            }
            
            # Validate backend result
            validation_result = model_manager.validate_backend_bdd_results(
                mock_backend_result, expected_result, test_ticket
            )
            
            # Assertions
            assert validation_result.validation_passed, f"BDD validation failed: {validation_result.differences}"
            assert validation_result.similarity_score > 0.6, "Similarity score too low"
            assert validation_result.structure_match, "BDD structure should be valid"
            
            logger.info(f"✅ End-to-end BDD validation passed with score: {validation_result.similarity_score:.2f}")

    def test_bdd_validation_test_suite(self, model_manager, test_data_generator):
        """Test running a complete BDD validation test suite."""
        
        # Create multiple test cases
        test_cases = []
        
        for i in range(3):
            jira_ticket = test_data_generator.generate_jira_ticket(
                key=f"TEST-{100+i}",
                summary=f"Feature {i+1} functionality",
                description=f"Implement feature {i+1} for testing",
                issue_type="Story"
            )
            
            backend_result = {
                "feature": f"""@TEST-{100+i} @AutoGenerated
Feature: Feature {i+1} functionality
  Scenario: Test scenario {i+1}
    Given test condition {i+1}
    When test action {i+1}
    Then test result {i+1}
""",
                "step_definitions": f"""import {{ Given, When, Then }} from '@wdio/cucumber-framework';
Given('test condition {i+1}', async () => {{ await browser.url('/test'); }});
""",
                "ticket_key": f"TEST-{100+i}"
            }
            
            expected_result = {
                "feature": f"""@TEST-{100+i} @AutoGenerated
Feature: Feature {i+1} functionality
  Scenario: Test scenario {i+1}
    Given test condition {i+1}
    When test action {i+1}
    Then test result {i+1}
""",
                "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';"""
            }
            
            test_cases.append({
                "test_id": f"bdd_test_{i+1}",
                "jira_ticket": jira_ticket,
                "backend_result": backend_result,
                "expected_result": expected_result
            })
        
        # Run validation test suite
        suite_results = model_manager.run_bdd_validation_test_suite(test_cases)
        
        # Assertions
        assert suite_results["total_tests"] == 3, "Should run 3 tests"
        assert suite_results["passed_tests"] > 0, "At least some tests should pass"
        assert "summary" in suite_results, "Should include summary"
        assert "success_rate" in suite_results["summary"], "Should calculate success rate"
        
        success_rate = suite_results["summary"]["success_rate"]
        logger.info(f"✅ BDD validation test suite completed with {success_rate:.1%} success rate")

    @pytest.mark.performance
    def test_bdd_validation_performance(self, model_manager, test_data_generator):
        """Test BDD validation performance with multiple scenarios."""
        
        # Generate test data
        jira_ticket = test_data_generator.generate_jira_ticket()
        
        backend_result = {
            "feature": """@TEST-123 @AutoGenerated
Feature: Performance test
  Scenario: Quick validation test
    Given fast condition
    When fast action
    Then fast result
""",
            "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';""",
            "ticket_key": "TEST-123"
        }
        
        expected_result = backend_result.copy()
        
        import time
        start_time = time.time()
        
        # Run validation multiple times
        for i in range(10):
            validation_result = model_manager.validate_backend_bdd_results(
                backend_result, expected_result, jira_ticket
            )
            assert validation_result.validation_passed, f"Validation {i+1} failed"
        
        total_time = time.time() - start_time
        avg_time = total_time / 10
        
        # Performance assertions
        assert avg_time < 2.0, f"Average validation time too slow: {avg_time:.2f}s"
        assert total_time < 15.0, f"Total validation time too slow: {total_time:.2f}s"
        
        logger.info(f"✅ BDD validation performance test passed: {avg_time:.3f}s average")


@pytest.mark.bdd
@pytest.mark.unit
class TestBDDValidationUnits:
    """Unit tests for individual BDD validation components."""
    
    @pytest.fixture
    def model_manager(self):
        return ModelManager()

    def test_text_similarity_calculation(self, model_manager):
        """Test text similarity calculation for BDD validation."""
        
        # Similar texts
        text1 = "Given user is on login page"
        text2 = "Given user is on the login page"
        
        similarity = model_manager._calculate_text_similarity(text1, text2)
        
        assert 0.0 <= similarity <= 1.0, "Similarity should be between 0 and 1"
        assert similarity > 0.8, f"Similar texts should have high similarity: {similarity}"
        
        # Different texts
        text3 = "Given user is on login page"
        text4 = "When user clicks checkout button"
        
        different_similarity = model_manager._calculate_text_similarity(text3, text4)
        assert different_similarity < similarity, "Different texts should have lower similarity"
        
        logger.info(f"✅ Text similarity test passed: similar={similarity:.3f}, different={different_similarity:.3f}")

    def test_model_loading_efficiency(self, model_manager):
        """Test that models load efficiently for BDD validation."""
        
        import time
        
        # Test CodeBERT loading
        start_time = time.time()
        model, tokenizer = model_manager.load_model("codebert")
        load_time = time.time() - start_time
        
        assert model is not None, "CodeBERT model should load successfully"
        assert tokenizer is not None, "CodeBERT tokenizer should load successfully"
        assert load_time < 30.0, f"Model loading too slow: {load_time:.2f}s"
        
        # Test sentence transformer loading
        start_time = time.time()
        st_model, st_tokenizer = model_manager.load_model("sentence-transformer")
        st_load_time = time.time() - start_time
        
        assert st_model is not None, "Sentence transformer should load successfully"
        assert st_load_time < 30.0, f"Sentence transformer loading too slow: {st_load_time:.2f}s"
        
        logger.info(f"✅ Model loading test passed: CodeBERT={load_time:.2f}s, SentenceTransformer={st_load_time:.2f}s")

    def test_essential_models_only(self, model_manager):
        """Test that only essential models are supported."""
        
        # Should support essential models
        assert "codebert" in model_manager.SUPPORTED_MODELS
        assert "sentence-transformer" in model_manager.SUPPORTED_MODELS
        
        # Should not support unnecessary models  
        unnecessary_models = ["t5-large", "gpt-3", "bert-large", "distilbert"]
        for model in unnecessary_models:
            assert model not in model_manager.SUPPORTED_MODELS, f"Unnecessary model {model} should not be supported"
        
        # Should have exactly 2 essential models
        assert len(model_manager.SUPPORTED_MODELS) == 2, f"Should have exactly 2 models, got {len(model_manager.SUPPORTED_MODELS)}"
        
        logger.info(f"✅ Essential models validation passed: {list(model_manager.SUPPORTED_MODELS.keys())}")