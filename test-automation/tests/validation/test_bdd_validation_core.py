"""
BDD Validation Tests - Core functionality for validating generated BDD against backend results
Focuses on the project scope: creating BDD scenarios and validating against actual backend data
"""

import pytest
import pytest_asyncio
from typing import Dict, Any
from unittest.mock import Mock, patch

from src.models.model_manager import ModelManager
from src.utils.test_helpers import TestDataGenerator
from src.backend_integration import BackendTestClient, AgentTestRunner
from src.config.logging import get_logger

logger = get_logger(__name__)

@pytest.mark.bdd
@pytest.mark.validation
class TestBDDValidationCore:
    """Test core BDD validation functionality against backend results"""
    
    @pytest.fixture
    def model_manager(self):
        """Initialize model manager for validation"""
        return ModelManager()
    
    @pytest.fixture
    def sample_backend_result(self):
        """Sample backend execution result"""
        return {
            "status": "completed",
            "ticket_key": "TEST-123",
            "execution_time": 2.5,
            "generated_tests": {
                "feature": "Feature: User authentication\n  Scenario: Login success",
                "step_definitions": "import { Given } from '@wdio/cucumber-framework';",
                "feature_file_name": "authentication.feature",
                "steps_file_name": "authentication.steps.ts"
            },
            "application_data": {
                "url": "https://app.example.com/login",
                "navigation_completed": True,
                "elements_found": ["#username", "#password", "button[type=submit]"]
            },
            "pr_urls": ["https://github.com/test/repo/pull/123"]
        }
    
    @pytest.fixture
    def sample_generated_bdd(self):
        """Sample generated BDD from backend"""
        return {
            "feature": """@TEST-123 @AutoGenerated
Feature: User authentication
  As a user
  I want to log into the application
  So that I can access my account

  Scenario: Successful login with valid credentials
    Given user is on login page
    When user enters valid username and password
    Then user should be redirected to dashboard
    And user should see welcome message

  Scenario: Failed login with invalid credentials
    Given user is on login page
    When user enters invalid credentials
    Then error message should be displayed
    And user should remain on login page
""",
            "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';

Given('user is on login page', async () => {
    await browser.url('/login');
    await expect($('#loginForm')).toBeDisplayed();
});

When('user enters valid username and password', async () => {
    await $('#username').setValue('testuser');
    await $('#password').setValue('testpass');
    await $('button[type="submit"]').click();
});

When('user enters invalid credentials', async () => {
    await $('#username').setValue('invalid');
    await $('#password').setValue('wrong');
    await $('button[type="submit"]').click();
});

Then('user should be redirected to dashboard', async () => {
    await expect(browser).toHaveUrl('/dashboard');
    await expect($('.dashboard')).toBeDisplayed();
});

Then('user should see welcome message', async () => {
    await expect($('.welcome-message')).toBeDisplayed();
});

Then('error message should be displayed', async () => {
    await expect($('.error-message')).toBeDisplayed();
});

Then('user should remain on login page', async () => {
    await expect(browser).toHaveUrl('/login');
});
""",
            "ticket_key": "TEST-123"
        }
    
    @pytest.fixture
    def sample_original_ticket(self):
        """Sample original JIRA ticket"""
        return {
            "key": "TEST-123",
            "summary": "User authentication system",
            "description": "Implement secure user login functionality for the application",
            "acceptance_criteria": """
                Given a registered user
                When they enter valid credentials
                Then they should be logged in successfully
                
                Given an unregistered user
                When they enter invalid credentials
                Then they should see an error message
            """,
            "issue_type": "Story",
            "components": ["frontend", "authentication"]
        }

    def test_validate_bdd_against_backend_result(self, model_manager, sample_generated_bdd, 
                                               sample_backend_result, sample_original_ticket):
        """Test core BDD validation against backend execution result"""
        
        # Mock the sentence transformer model to avoid loading during test
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
            
            # Mock the encode method to return predictable embeddings
            mock_model = model_manager.loaded_models["sentence-transformer"]
            mock_model.encode.return_value = [
                [0.1, 0.2, 0.3, 0.4],  # requirement embedding
                [0.15, 0.25, 0.35, 0.45]  # BDD content embedding
            ]
            
            # Execute validation
            validation_result = model_manager.validate_bdd_against_backend_result(
                sample_generated_bdd,
                sample_backend_result,
                sample_original_ticket
            )
            
            # Verify validation structure
            assert "overall_score" in validation_result
            assert "requirement_coverage" in validation_result
            assert "scenario_completeness" in validation_result
            assert "step_accuracy" in validation_result
            assert "backend_alignment" in validation_result
            assert "recommendations" in validation_result
            
            # Verify scores are within valid range
            assert 0.0 <= validation_result["overall_score"] <= 1.0
            assert 0.0 <= validation_result["requirement_coverage"] <= 1.0
            assert 0.0 <= validation_result["scenario_completeness"] <= 1.0
            assert 0.0 <= validation_result["step_accuracy"] <= 1.0
            assert 0.0 <= validation_result["backend_alignment"] <= 1.0
            
            # Verify recommendations are provided
            assert isinstance(validation_result["recommendations"], list)
            assert len(validation_result["recommendations"]) > 0
            
            logger.info(f"BDD validation completed with score: {validation_result['overall_score']:.2f}")

    def test_requirement_coverage_validation(self, model_manager, sample_generated_bdd, sample_original_ticket):
        """Test requirement coverage validation specifically"""
        
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
            
            # Mock high similarity between requirements and BDD
            mock_model = model_manager.loaded_models["sentence-transformer"]
            mock_model.encode.return_value = [
                [0.1, 0.2, 0.3, 0.4],
                [0.12, 0.22, 0.32, 0.42]  # High similarity
            ]
            
            # Test requirement coverage
            coverage_score = model_manager._validate_requirement_coverage(
                sample_generated_bdd, sample_original_ticket
            )
            
            assert 0.0 <= coverage_score <= 1.0
            assert coverage_score > 0.5  # Should have decent coverage with similar embeddings
            
            logger.info(f"Requirement coverage score: {coverage_score:.2f}")

    def test_scenario_completeness_validation(self, model_manager, sample_generated_bdd):
        """Test BDD scenario completeness validation"""
        
        completeness_score = model_manager._validate_scenario_completeness(sample_generated_bdd)
        
        assert 0.0 <= completeness_score <= 1.0
        
        # With well-formed BDD, should score high
        assert completeness_score > 0.7
        
        logger.info(f"Scenario completeness score: {completeness_score:.2f}")

    def test_step_accuracy_validation(self, model_manager, sample_generated_bdd):
        """Test step definition accuracy validation"""
        
        accuracy_score = model_manager._validate_step_accuracy(sample_generated_bdd)
        
        assert 0.0 <= accuracy_score <= 1.0
        
        # With proper step definitions, should score high
        assert accuracy_score > 0.6
        
        logger.info(f"Step accuracy score: {accuracy_score:.2f}")

    def test_backend_alignment_validation(self, model_manager, sample_generated_bdd, sample_backend_result):
        """Test backend alignment validation"""
        
        alignment_score = model_manager._validate_backend_alignment(
            sample_generated_bdd, sample_backend_result
        )
        
        assert 0.0 <= alignment_score <= 1.0
        
        # With successful backend result and proper BDD, should align well
        assert alignment_score > 0.5
        
        logger.info(f"Backend alignment score: {alignment_score:.2f}")

    def test_bdd_comparison_with_expected(self, model_manager, sample_generated_bdd):
        """Test comparing generated BDD with expected BDD"""
        
        expected_bdd = {
            "feature": """Feature: User authentication
  Scenario: Login functionality
    Given user is on login page
    When user enters credentials
    Then user should be authenticated
""",
            "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';
Given('user is on login page', async () => {
    await browser.url('/login');
});
"""
        }
        
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
            
            # Mock similarity calculation
            mock_model = model_manager.loaded_models["sentence-transformer"]
            mock_model.encode.return_value = [
                [0.1, 0.2, 0.3, 0.4],  # Generated feature
                [0.15, 0.25, 0.35, 0.45],  # Expected feature
                [0.2, 0.3, 0.4, 0.5],  # Generated steps
                [0.25, 0.35, 0.45, 0.55]  # Expected steps
            ]
            
            comparison_result = model_manager.compare_bdd_with_expected(
                sample_generated_bdd, expected_bdd
            )
            
            assert "feature_similarity" in comparison_result
            assert "step_similarity" in comparison_result
            assert "overall_similarity" in comparison_result
            assert "differences" in comparison_result
            assert "improvements" in comparison_result
            
            # Verify similarity scores are valid
            assert 0.0 <= comparison_result["feature_similarity"] <= 1.0
            assert 0.0 <= comparison_result["step_similarity"] <= 1.0
            assert 0.0 <= comparison_result["overall_similarity"] <= 1.0
            
            logger.info(f"BDD comparison completed with similarity: {comparison_result['overall_similarity']:.2f}")

    def test_validation_with_poor_bdd(self, model_manager):
        """Test validation with poorly formed BDD"""
        
        poor_bdd = {
            "feature": "Feature: Incomplete",  # Missing scenarios
            "step_definitions": "// TODO: implement steps",  # No implementation
            "ticket_key": "TEST-456"
        }
        
        backend_result = {
            "status": "completed",
            "ticket_key": "TEST-456"
        }
        
        original_ticket = {
            "key": "TEST-456",
            "summary": "Complex feature",
            "description": "Detailed requirements",
            "acceptance_criteria": "Multiple criteria"
        }
        
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
            
            # Mock low similarity
            mock_model = model_manager.loaded_models["sentence-transformer"]
            mock_model.encode.return_value = [
                [0.1, 0.2, 0.3, 0.4],  # requirement
                [0.9, 0.8, 0.7, 0.6]   # BDD (very different)
            ]
            
            validation_result = model_manager.validate_bdd_against_backend_result(
                poor_bdd, backend_result, original_ticket
            )
            
            # Poor BDD should score low
            assert validation_result["overall_score"] < 0.5
            assert validation_result["scenario_completeness"] < 0.5
            assert validation_result["step_accuracy"] < 0.5
            
            # Should have improvement recommendations
            assert len(validation_result["recommendations"]) > 0
            assert any("improvement" in rec.lower() for rec in validation_result["recommendations"])
            
            logger.info(f"Poor BDD validation score: {validation_result['overall_score']:.2f}")

    def test_validation_error_handling(self, model_manager):
        """Test validation error handling with invalid inputs"""
        
        # Test with empty inputs
        validation_result = model_manager.validate_bdd_against_backend_result(
            {}, {}, {}
        )
        
        # Should handle gracefully
        assert "overall_score" in validation_result
        assert validation_result["overall_score"] == 0.0


@pytest.mark.bdd
@pytest.mark.integration
class TestBDDValidationIntegration:
    """Integration tests for BDD validation with backend"""
    
    @pytest.fixture
    def backend_client(self):
        """Backend test client"""
        return BackendTestClient()
    
    @pytest.fixture
    def agent_runner(self):
        """Agent test runner"""
        return AgentTestRunner()

    @pytest.mark.asyncio
    async def test_end_to_end_bdd_validation(self, backend_client, agent_runner):
        """Test complete end-to-end BDD validation flow"""
        
        # Setup test scenario
        test_ticket = {
            "key": "E2E-001",
            "summary": "User profile management",
            "description": "Allow users to update their profile information",
            "acceptance_criteria": "User can edit and save profile data",
            "issue_type": "Story"
        }
        
        # Mock backend workflow execution
        with patch('backend.src.agents.orchestrator.WorkflowOrchestrator') as mock_orchestrator_class:
            mock_orchestrator = Mock()
            mock_orchestrator.process_tickets.return_value = {
                'status': 'completed',
                'result': {
                    'generated_tests': {
                        'feature': '@E2E-001 @AutoGenerated\nFeature: User profile\n  Scenario: Update profile',
                        'step_definitions': 'import { Given } from "@wdio/cucumber-framework";',
                        'ticket_key': 'E2E-001'
                    }
                }
            }
            mock_orchestrator_class.return_value = mock_orchestrator
            
            # Initialize validation components
            await backend_client.initialize_async_client()
            model_manager = ModelManager()
            
            try:
                # Mock model loading for validation
                with patch.object(model_manager, '_is_model_loaded', return_value=True), \
                     patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
                    
                    # Mock similarity calculation
                    mock_model = model_manager.loaded_models["sentence-transformer"]
                    mock_model.encode.return_value = [
                        [0.1, 0.2, 0.3, 0.4],  # requirement embedding
                        [0.12, 0.22, 0.32, 0.42]  # BDD embedding (high similarity)
                    ]
                    
                    # Execute workflow and get result
                    workflow_result = await backend_client.trigger_workflow_test([test_ticket["key"]])
                    
                    # Extract generated BDD from workflow result
                    generated_bdd = {
                        "feature": '@E2E-001 @AutoGenerated\nFeature: User profile\n  Scenario: Update profile',
                        "step_definitions": 'import { Given } from "@wdio/cucumber-framework";',
                        "ticket_key": 'E2E-001'
                    }
                    
                    # Validate generated BDD against backend result
                    validation_result = model_manager.validate_bdd_against_backend_result(
                        generated_bdd,
                        {"status": "completed", "ticket_key": "E2E-001"},
                        test_ticket
                    )
                    
                    # Verify end-to-end validation
                    assert validation_result["overall_score"] > 0.0
                    assert "recommendations" in validation_result
                    assert len(validation_result["recommendations"]) > 0
                    
                    logger.info(f"End-to-end validation completed: {validation_result['overall_score']:.2f}")
                    
            finally:
                await backend_client.close_async_client()

    @pytest.mark.asyncio 
    async def test_validation_against_actual_backend_patterns(self, model_manager):
        """Test validation against actual backend execution patterns"""
        
        # Simulate real backend result with navigation
        backend_result_with_navigation = {
            "status": "completed",
            "ticket_key": "NAV-001",
            "application_data": {
                "url": "https://app.example.com/users",
                "navigation_completed": True,
                "elements": [
                    {"type": "form", "id": "userForm"},
                    {"type": "input", "name": "firstName"},
                    {"type": "button", "text": "Save"}
                ]
            },
            "execution_time": 3.2
        }
        
        # BDD that matches navigation pattern
        navigation_bdd = {
            "feature": """@NAV-001 @AutoGenerated
Feature: User profile editing
  Scenario: Edit user profile
    Given user navigates to profile page
    When user updates first name
    And user clicks save button
    Then profile should be updated
""",
            "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';

Given('user navigates to profile page', async () => {
    await browser.url('/users');
    await expect($('#userForm')).toBeDisplayed();
});

When('user updates first name', async () => {
    await $('input[name="firstName"]').setValue('Updated Name');
});

When('user clicks save button', async () => {
    await $('button:contains("Save")').click();
});
""",
            "ticket_key": "NAV-001"
        }
        
        original_ticket = {
            "key": "NAV-001",
            "summary": "User profile editing",
            "description": "Allow users to edit their profile information",
            "acceptance_criteria": "User can navigate to profile page and update information"
        }
        
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
            
            # Mock good similarity
            mock_model = model_manager.loaded_models["sentence-transformer"]
            mock_model.encode.return_value = [
                [0.1, 0.2, 0.3, 0.4],
                [0.11, 0.21, 0.31, 0.41]
            ]
            
            validation_result = model_manager.validate_bdd_against_backend_result(
                navigation_bdd,
                backend_result_with_navigation,
                original_ticket
            )
            
            # Should score well due to navigation pattern match
            assert validation_result["backend_alignment"] > 0.7
            assert validation_result["overall_score"] > 0.6
            
            logger.info(f"Navigation pattern validation: {validation_result['backend_alignment']:.2f}")


@pytest.mark.bdd
@pytest.mark.performance
class TestBDDValidationPerformance:
    """Performance tests for BDD validation"""
    
    def test_validation_performance(self, model_manager):
        """Test validation performance with realistic data"""
        
        # Large BDD scenario
        large_bdd = {
            "feature": "Feature: Complex application\n" + "  Scenario: Test scenario\n" * 10,
            "step_definitions": "import test\n" + "Given('step', () => {});\n" * 20,
            "ticket_key": "PERF-001"
        }
        
        backend_result = {"status": "completed", "ticket_key": "PERF-001"}
        ticket = {"key": "PERF-001", "summary": "Test", "description": "Test"}
        
        import time
        start_time = time.time()
        
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
            
            mock_model = model_manager.loaded_models["sentence-transformer"]
            mock_model.encode.return_value = [[0.1, 0.2], [0.1, 0.2]]
            
            validation_result = model_manager.validate_bdd_against_backend_result(
                large_bdd, backend_result, ticket
            )
        
        execution_time = time.time() - start_time
        
        # Validation should complete quickly
        assert execution_time < 1.0  # Should complete in under 1 second
        assert validation_result["overall_score"] >= 0.0
        
        logger.info(f"Validation performance: {execution_time:.3f}s")