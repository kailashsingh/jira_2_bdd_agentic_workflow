"""
Integration tests for the JIRA to BDD workflow orchestration.
Tests the complete pipeline from JIRA ticket retrieval to BDD generation and PR creation.
"""

import pytest
import asyncio
from unittest.mock import Mock, AsyncMock, patch
from typing import Dict, List, Any
import json
import tempfile
from pathlib import Path

from src.utils.test_helpers import (
    TestDataGenerator, 
    MockFactory, 
    TestFileManager,
    PerformanceTimer,
    AsyncTestHelper,
    ValidationHelper
)

# Import the actual classes (mocked in conftest.py)
try:
    from src.agents.orchestrator import WorkflowOrchestrator
    from src.agents.bdd_generator_agent import BDDGeneratorAgent
    from src.tools.jira_tools import JiraTools
    from src.tools.github_tools import GitHubTools
    from src.tools.rag_tools import RAGTools
    from src.tools.application_tools import ApplicationTools
except ImportError:
    # Fallback for testing environment
    WorkflowOrchestrator = Mock
    BDDGeneratorAgent = Mock
    JiraTools = Mock
    GitHubTools = Mock
    RAGTools = Mock
    ApplicationTools = Mock

class TestWorkflowOrchestration:
    """Test suite for workflow orchestration."""
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup test environment."""
        self.data_generator = TestDataGenerator()
        self.file_manager = TestFileManager()
        self.test_tickets = self.data_generator.generate_test_suite(3)
        
        yield
        
        # Cleanup
        self.file_manager.cleanup()
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_complete_workflow_single_ticket_with_validation(self, 
                                                                 mock_jira_tools,
                                                                 mock_github_tools,
                                                                 mock_rag_tools,
                                                                 mock_application_tools,
                                                                 mock_bdd_generator,
                                                                 model_manager):
        """Test complete workflow for a single ticket with BDD validation against backend results."""
        # Arrange
        test_ticket = self.test_tickets[0]
        ticket_data = {
            'key': test_ticket.key,
            'summary': test_ticket.summary,
            'description': test_ticket.description,
            'acceptance_criteria': test_ticket.acceptance_criteria,
            'components': test_ticket.components
        }
        
        # Mock generated BDD result
        generated_bdd = {
            'feature': f'@{test_ticket.key} @AutoGenerated\nFeature: {test_ticket.summary}\n  Scenario: Test scenario',
            'step_definitions': 'import { Given } from "@wdio/cucumber-framework";',
            'ticket_key': test_ticket.key,
            'feature_file_name': f'{test_ticket.key.lower()}.feature',
            'steps_file_name': f'{test_ticket.key.lower()}.steps.ts'
        }
        
        mock_jira_tools.get_tickets.return_value = [ticket_data]
        mock_bdd_generator.generate_bdd_scenarios.return_value = generated_bdd
        
        # Mock the workflow orchestrator with our mocks
        with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
             patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
             patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
             patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
             patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
            
            orchestrator = WorkflowOrchestrator()
            
            # Act
            with PerformanceTimer("single_ticket_workflow") as timer:
                result = await orchestrator.process_tickets([test_ticket.key])
            
            # Assert basic workflow completion
            assert result['status'] == 'completed'
            assert 'result' in result
            
            # Verify workflow steps were called
            mock_jira_tools.get_tickets.assert_called_once_with([test_ticket.key], None)
            mock_rag_tools.index_codebase.assert_called_once()
            mock_bdd_generator.is_testable.assert_called_once()
            mock_bdd_generator.generate_bdd_scenarios.assert_called_once()
            mock_github_tools.create_pull_request.assert_called_once()
            
            # Validate generated BDD against backend result
            backend_result = {
                'status': 'completed',
                'ticket_key': test_ticket.key,
                'generated_tests': generated_bdd,
                'execution_time': timer.duration
            }
            
            # Mock model loading for validation
            with patch.object(model_manager, '_is_model_loaded', return_value=True), \
                 patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
                
                # Mock similarity calculation for validation
                mock_model = model_manager.loaded_models["sentence-transformer"]
                mock_model.encode.return_value = [
                    [0.1, 0.2, 0.3, 0.4],  # requirement embedding
                    [0.12, 0.22, 0.32, 0.42]  # BDD embedding (high similarity)
                ]
                
                validation_result = model_manager.validate_bdd_against_backend_result(
                    generated_bdd, backend_result, ticket_data
                )
                
                # Assert validation passed
                assert validation_result["overall_score"] > 0.0
                assert "requirement_coverage" in validation_result
                assert "backend_alignment" in validation_result
                assert validation_result["backend_alignment"] > 0.5
            
            # Performance assertion
            assert timer.duration < 10.0, f"Workflow took too long: {timer.duration}s"
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_workflow_validation_against_actual_backend_data(self,
                                                                 mock_jira_tools,
                                                                 mock_github_tools,
                                                                 mock_rag_tools,
                                                                 mock_application_tools,
                                                                 mock_bdd_generator,
                                                                 model_manager):
        """Test workflow validation against actual backend execution data."""
        # Arrange - Create realistic backend execution scenario
        test_ticket = self.test_tickets[0]
        ticket_data = {
            'key': test_ticket.key,
            'summary': 'User login functionality',
            'description': 'Implement user authentication with email and password',
            'acceptance_criteria': 'User can login with valid credentials and see error for invalid ones',
            'components': ['frontend', 'authentication']
        }
        
        # Mock realistic backend execution result with application data
        backend_execution_result = {
            'status': 'completed',
            'ticket_key': test_ticket.key,
            'execution_time': 2.5,
            'application_data': {
                'url': 'https://app.example.com/login',
                'navigation_completed': True,
                'elements_found': ['#email', '#password', 'button[type="submit"]'],
                'form_validation': True,
                'page_title': 'Login - MyApp'
            },
            'generated_tests': {
                'feature': f'''@{test_ticket.key} @AutoGenerated
Feature: User login functionality
  As a user
  I want to login with my credentials
  So that I can access my account

  Scenario: Successful login with valid credentials
    Given user is on login page
    When user enters valid email and password
    And user clicks login button
    Then user should be redirected to dashboard
    And user should see welcome message

  Scenario: Failed login with invalid credentials
    Given user is on login page
    When user enters invalid credentials
    And user clicks login button
    Then error message should be displayed
    And user should remain on login page
''',
                'step_definitions': '''import { Given, When, Then } from '@wdio/cucumber-framework';

Given('user is on login page', async () => {{
    await browser.url('/login');
    await expect($('#loginForm')).toBeDisplayed();
    await expect(browser).toHaveTitle('Login - MyApp');
}});

When('user enters valid email and password', async () => {{
    await $('#email').setValue('user@example.com');
    await $('#password').setValue('validpass123');
}});

When('user enters invalid credentials', async () => {{
    await $('#email').setValue('invalid@example.com');
    await $('#password').setValue('wrongpass');
}});

When('user clicks login button', async () => {{
    await $('button[type="submit"]').click();
}});

Then('user should be redirected to dashboard', async () => {{
    await expect(browser).toHaveUrl('/dashboard');
    await expect($('.dashboard')).toBeDisplayed();
}});

Then('user should see welcome message', async () => {{
    await expect($('.welcome-message')).toBeDisplayed();
}});

Then('error message should be displayed', async () => {{
    await expect($('.error-message')).toBeDisplayed();
}});

Then('user should remain on login page', async () => {{
    await expect(browser).toHaveUrl('/login');
}});
''',
                'feature_file_name': f'{test_ticket.key.lower()}_login.feature',
                'steps_file_name': f'{test_ticket.key.lower()}_login.steps.ts'
            },
            'pr_urls': ['https://github.com/test/repo/pull/456']
        }
        
        mock_jira_tools.get_tickets.return_value = [ticket_data]
        mock_bdd_generator.generate_bdd_scenarios.return_value = backend_execution_result['generated_tests']
        mock_application_tools.navigate_and_collect_data_using_mcp.return_value = backend_execution_result['application_data']
        
        with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
             patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
             patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
             patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
             patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
            
            orchestrator = WorkflowOrchestrator()
            
            # Act - Execute workflow
            workflow_result = await orchestrator.process_tickets([test_ticket.key])
            
            # Assert workflow completed
            assert workflow_result['status'] == 'completed'
            
            # Validate generated BDD against actual backend data
            with patch.object(model_manager, '_is_model_loaded', return_value=True), \
                 patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
                
                # Mock similarity for requirement coverage validation
                mock_model = model_manager.loaded_models["sentence-transformer"]
                mock_model.encode.return_value = [
                    [0.1, 0.2, 0.3, 0.4],  # requirement embedding
                    [0.13, 0.23, 0.33, 0.43]  # BDD feature embedding (good similarity)
                ]
                
                # Validate BDD matches backend execution results
                validation_result = model_manager.validate_bdd_against_backend_result(
                    backend_execution_result['generated_tests'],
                    backend_execution_result,
                    ticket_data
                )
                
                # Assert validation quality
                assert validation_result["overall_score"] > 0.6, "BDD should have good overall score"
                assert validation_result["requirement_coverage"] > 0.7, "Should cover requirements well"
                assert validation_result["backend_alignment"] > 0.8, "Should align well with backend data"
                assert validation_result["scenario_completeness"] > 0.7, "Scenarios should be complete"
                assert validation_result["step_accuracy"] > 0.6, "Steps should be accurate"
                
                # Verify backend data alignment specifics
                assert len(validation_result["recommendations"]) >= 0
                
                # Test specific backend data validation
                app_data = backend_execution_result['application_data']
                generated_feature = backend_execution_result['generated_tests']['feature']
                generated_steps = backend_execution_result['generated_tests']['step_definitions']
                
                # Verify URL alignment
                assert '/login' in generated_steps, "Should reference login URL from backend data"
                
                # Verify element alignment  
                for element in app_data['elements_found']:
                    element_name = element.replace('#', '').replace('[type="submit"]', '')
                    if element_name in ['email', 'password']:
                        assert element_name in generated_steps, f"Should reference {element_name} element"
                
                # Verify page title alignment
                if app_data.get('page_title'):
                    assert 'Login' in generated_steps, "Should reference page title information"
                
                print(f"Validation completed with score: {validation_result['overall_score']:.2f}")
                print(f"Backend alignment: {validation_result['backend_alignment']:.2f}")
                print(f"Requirement coverage: {validation_result['requirement_coverage']:.2f}")
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_workflow_multiple_tickets(self,
                                           mock_jira_tools,
                                           mock_github_tools, 
                                           mock_rag_tools,
                                           mock_application_tools,
                                           mock_bdd_generator):
        """Test workflow with multiple tickets."""
        # Arrange
        ticket_keys = [t.key for t in self.test_tickets]
        ticket_data = []
        for ticket in self.test_tickets:
            ticket_data.append({
                'key': ticket.key,
                'summary': ticket.summary,
                'description': ticket.description,
                'acceptance_criteria': ticket.acceptance_criteria,
                'components': ticket.components
            })
        
        mock_jira_tools.get_tickets.return_value = ticket_data
        
        with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
             patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
             patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
             patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
             patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
            
            orchestrator = WorkflowOrchestrator()
            
            # Act
            with PerformanceTimer("multiple_tickets_workflow") as timer:
                result = await orchestrator.process_tickets(ticket_keys)
            
            # Assert
            assert result['status'] == 'completed'
            
            # Verify all tickets were processed
            assert mock_bdd_generator.is_testable.call_count == len(self.test_tickets)
            assert mock_bdd_generator.generate_bdd_scenarios.call_count == len(self.test_tickets)
            assert mock_github_tools.create_pull_request.call_count == len(self.test_tickets)
            
            # Performance should scale reasonably
            expected_max_time = len(self.test_tickets) * 5.0  # 5s per ticket max
            assert timer.duration < expected_max_time
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_workflow_with_non_testable_ticket(self,
                                                   mock_jira_tools,
                                                   mock_github_tools,
                                                   mock_rag_tools, 
                                                   mock_application_tools,
                                                   mock_bdd_generator):
        """Test workflow skips non-testable tickets appropriately."""
        # Arrange
        test_ticket = self.test_tickets[0]
        ticket_data = {
            'key': test_ticket.key,
            'summary': test_ticket.summary,
            'description': test_ticket.description,
            'acceptance_criteria': test_ticket.acceptance_criteria,
            'components': test_ticket.components
        }
        
        mock_jira_tools.get_tickets.return_value = [ticket_data]
        mock_bdd_generator.is_testable.return_value = False  # Make it non-testable
        
        with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
             patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
             patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
             patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
             patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
            
            orchestrator = WorkflowOrchestrator()
            
            # Act
            result = await orchestrator.process_tickets([test_ticket.key])
            
            # Assert
            assert result['status'] == 'completed'
            
            # Verify ticket was checked but no BDD was generated
            mock_bdd_generator.is_testable.assert_called_once()
            mock_bdd_generator.generate_bdd_scenarios.assert_not_called()
            mock_github_tools.create_pull_request.assert_not_called()
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_workflow_error_handling(self,
                                         mock_jira_tools,
                                         mock_github_tools,
                                         mock_rag_tools,
                                         mock_application_tools,
                                         mock_bdd_generator):
        """Test workflow handles errors gracefully."""
        # Arrange - simulate JIRA error
        mock_jira_tools.get_tickets.side_effect = Exception("JIRA connection failed")
        
        with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
             patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
             patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
             patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
             patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
            
            orchestrator = WorkflowOrchestrator()
            
            # Act
            result = await orchestrator.process_tickets(["TEST-ERROR"])
            
            # Assert
            assert result['status'] == 'error'
            assert 'message' in result
            assert 'JIRA connection failed' in result['message'] or 'Unexpected error' in result['message']
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_workflow_application_navigation(self,
                                                 mock_jira_tools,
                                                 mock_github_tools,
                                                 mock_rag_tools,
                                                 mock_application_tools,
                                                 mock_bdd_generator):
        """Test workflow with application navigation."""
        # Arrange
        test_ticket = self.test_tickets[0]
        ticket_data = {
            'key': test_ticket.key,
            'summary': test_ticket.summary,
            'description': test_ticket.description,
            'acceptance_criteria': test_ticket.acceptance_criteria,
            'components': test_ticket.components
        }
        
        mock_jira_tools.get_tickets.return_value = [ticket_data]
        mock_application_tools.needs_navigation.return_value = True
        
        with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
             patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
             patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
             patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
             patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
            
            orchestrator = WorkflowOrchestrator()
            
            # Act
            result = await orchestrator.process_tickets([test_ticket.key])
            
            # Assert
            assert result['status'] == 'completed'
            
            # Verify navigation was performed
            mock_application_tools.needs_navigation.assert_called()
            mock_application_tools.navigate_and_collect_data_using_mcp.assert_called_once()
    
    @pytest.mark.integration 
    @pytest.mark.smoke
    @pytest.mark.asyncio
    async def test_workflow_sprint_mode(self,
                                      mock_jira_tools,
                                      mock_github_tools,
                                      mock_rag_tools,
                                      mock_application_tools,
                                      mock_bdd_generator):
        """Test workflow in sprint mode."""
        # Arrange
        sprint_id = 123
        ticket_data = []
        for ticket in self.test_tickets:
            ticket_data.append({
                'key': ticket.key,
                'summary': ticket.summary,
                'description': ticket.description,
                'acceptance_criteria': ticket.acceptance_criteria,
                'components': ticket.components
            })
        
        mock_jira_tools.get_sprint_tickets.return_value = ticket_data
        
        with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
             patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
             patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
             patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
             patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
            
            orchestrator = WorkflowOrchestrator()
            
            # Act
            result = await orchestrator.run(sprint_id)
            
            # Assert
            mock_jira_tools.get_sprint_tickets.assert_called_once_with(sprint_id)
            # Result structure depends on implementation
            assert isinstance(result, dict)

class TestWorkflowComponents:
    """Test individual workflow components."""
    
    @pytest.mark.integration
    def test_bdd_generator_integration(self, mock_rag_tools, sample_jira_ticket):
        """Test BDD generator with mocked dependencies."""
        with patch('src.agents.bdd_generator_agent.RAGTools', return_value=mock_rag_tools):
            generator = BDDGeneratorAgent(mock_rag_tools)
            
            # Test testability check
            is_testable = generator.is_testable(sample_jira_ticket)
            assert isinstance(is_testable, bool)
            
            # Test BDD generation
            if is_testable:
                similar_code = [{"content": "test code", "score": 0.9}]
                result = generator.generate_bdd_scenarios(
                    sample_jira_ticket, 
                    similar_code, 
                    "application data"
                )
                
                # Validate result structure
                required_keys = ['ticket_key', 'feature_file_name', 'steps_file_name', 'feature', 'step_definitions']
                for key in required_keys:
                    assert key in result
                
                # Validate content
                validation_result = ValidationHelper.validate_bdd_feature(result['feature'])
                assert validation_result['is_valid']
                
                steps_validation = ValidationHelper.validate_step_definitions(result['step_definitions'])
                assert steps_validation['is_valid']
    
    @pytest.mark.integration
    def test_jira_tools_integration(self, mock_jira_tools):
        """Test JIRA tools integration."""
        # Test sprint tickets retrieval
        tickets = mock_jira_tools.get_sprint_tickets(123)
        assert isinstance(tickets, list)
        
        # Test specific tickets retrieval
        specific_tickets = mock_jira_tools.get_tickets(["TEST-123", "TEST-456"])
        assert isinstance(specific_tickets, list)
        
        # Test ticket update
        result = mock_jira_tools.update_ticket_comment("TEST-123", "Test comment")
        assert result is True
    
    @pytest.mark.integration
    def test_github_tools_integration(self, mock_github_tools):
        """Test GitHub tools integration."""
        # Test feature files retrieval
        features = mock_github_tools.get_feature_files()
        assert isinstance(features, list)
        
        # Test step definitions retrieval
        steps = mock_github_tools.get_step_definitions()
        assert isinstance(steps, list)
        
        # Test branch creation
        result = mock_github_tools.create_branch("test-branch")
        assert result is True
        
        # Test PR creation
        pr_url = mock_github_tools.create_pull_request("test-branch", "Test PR", "Description")
        assert pr_url.startswith("https://github.com/")
    
    @pytest.mark.integration
    def test_rag_tools_integration(self, mock_rag_tools):
        """Test RAG tools integration."""
        # Test code search
        results = mock_rag_tools.search_similar_code("test query")
        assert isinstance(results, list)
        assert all('content' in result and 'score' in result for result in results)
        
        # Test codebase indexing
        result = mock_rag_tools.index_codebase(["feature1"], ["steps1"], Mock())
        assert result is True

class TestWorkflowPerformance:
    """Performance tests for workflow operations."""
    
    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_workflow_performance_benchmarks(self,
                                                 mock_jira_tools,
                                                 mock_github_tools,
                                                 mock_rag_tools,
                                                 mock_application_tools,
                                                 mock_bdd_generator,
                                                 benchmark_suite):
        """Benchmark workflow performance with different ticket complexities."""
        performance_results = {}
        
        for test_case in benchmark_suite["test_cases"]:
            ticket_data = test_case["ticket"]
            
            mock_jira_tools.get_tickets.return_value = [ticket_data]
            
            with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
                 patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
                 patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
                 patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
                 patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
                
                orchestrator = WorkflowOrchestrator()
                
                with PerformanceTimer(test_case["name"]) as timer:
                    result = await orchestrator.process_tickets([ticket_data["key"]])
                
                performance_results[test_case["name"]] = {
                    "duration": timer.duration,
                    "complexity": test_case["complexity"],
                    "success": result["status"] == "completed"
                }
        
        # Analyze performance results
        simple_time = performance_results.get("simple_ticket", {}).get("duration", 0)
        complex_time = performance_results.get("complex_ticket", {}).get("duration", 0)
        
        # Complex tickets should not take more than 3x simple tickets
        if simple_time > 0 and complex_time > 0:
            assert complex_time / simple_time < 3.0, "Complex tickets taking too much longer"
        
        # All should complete within reasonable time
        for result in performance_results.values():
            assert result["duration"] < 30.0, "Workflow taking too long"
            assert result["success"], "Workflow should complete successfully"
    
    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_concurrent_workflow_execution(self,
                                               mock_jira_tools,
                                               mock_github_tools,
                                               mock_rag_tools,
                                               mock_application_tools,
                                               mock_bdd_generator):
        """Test concurrent execution of workflows."""
        # Arrange
        data_generator = TestDataGenerator()
        test_tickets = data_generator.generate_test_suite(5)
        
        async def run_single_workflow(ticket):
            ticket_data = {
                'key': ticket.key,
                'summary': ticket.summary,
                'description': ticket.description,
                'acceptance_criteria': ticket.acceptance_criteria,
                'components': ticket.components
            }
            
            mock_jira_tools.get_tickets.return_value = [ticket_data]
            
            with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
                 patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
                 patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
                 patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
                 patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
                
                orchestrator = WorkflowOrchestrator()
                return await orchestrator.process_tickets([ticket.key])
        
        # Act
        with PerformanceTimer("concurrent_workflows") as timer:
            tasks = [run_single_workflow(ticket) for ticket in test_tickets]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Assert
        successful_results = [r for r in results if isinstance(r, dict) and r.get('status') == 'completed']
        
        assert len(successful_results) == len(test_tickets), "All workflows should complete"
        
        # Concurrent execution should be faster than sequential
        max_sequential_time = len(test_tickets) * 10.0  # 10s per ticket sequentially
        assert timer.duration < max_sequential_time, "Concurrent execution should be faster"

class TestWorkflowEdgeCases:
    """Test edge cases and error scenarios."""
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_empty_ticket_list(self,
                                   mock_jira_tools,
                                   mock_github_tools,
                                   mock_rag_tools,
                                   mock_application_tools,
                                   mock_bdd_generator):
        """Test workflow with empty ticket list."""
        mock_jira_tools.get_tickets.return_value = []
        
        with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
             patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
             patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
             patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
             patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
            
            orchestrator = WorkflowOrchestrator()
            result = await orchestrator.process_tickets([])
            
            # Should handle gracefully
            assert result['status'] in ['completed', 'error']
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_invalid_ticket_key(self,
                                    mock_jira_tools,
                                    mock_github_tools,
                                    mock_rag_tools,
                                    mock_application_tools,
                                    mock_bdd_generator):
        """Test workflow with invalid ticket key."""
        mock_jira_tools.get_tickets.side_effect = ValueError("Invalid ticket key: INVALID-999")
        
        with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
             patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
             patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
             patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
             patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
            
            orchestrator = WorkflowOrchestrator()
            result = await orchestrator.process_tickets(["INVALID-999"])
            
            assert result['status'] == 'error'
            assert 'Invalid ticket key' in result['message']
    
    @pytest.mark.integration
    @pytest.mark.asyncio
    async def test_github_rate_limit_error(self,
                                         mock_jira_tools,
                                         mock_github_tools,
                                         mock_rag_tools,
                                         mock_application_tools,
                                         mock_bdd_generator):
        """Test workflow handles GitHub rate limiting."""
        # Arrange
        data_generator = TestDataGenerator()
        test_ticket = data_generator.generate_jira_ticket()
        ticket_data = {
            'key': test_ticket.key,
            'summary': test_ticket.summary,
            'description': test_ticket.description,
            'acceptance_criteria': test_ticket.acceptance_criteria,
            'components': test_ticket.components
        }
        
        mock_jira_tools.get_tickets.return_value = [ticket_data]
        mock_github_tools.create_pull_request.side_effect = Exception("API rate limit exceeded")
        
        with patch('src.agents.orchestrator.JiraTools', return_value=mock_jira_tools), \
             patch('src.agents.orchestrator.GitHubTools', return_value=mock_github_tools), \
             patch('src.agents.orchestrator.RAGTools', return_value=mock_rag_tools), \
             patch('src.agents.orchestrator.ApplicationTools', return_value=mock_application_tools), \
             patch('src.agents.orchestrator.BDDGeneratorAgent', return_value=mock_bdd_generator):
            
            orchestrator = WorkflowOrchestrator()
            result = await orchestrator.process_tickets([test_ticket.key])
            
            # Should handle the error gracefully
            assert result['status'] == 'error'