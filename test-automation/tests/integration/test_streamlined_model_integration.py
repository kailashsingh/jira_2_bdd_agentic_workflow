"""
Integration tests for streamlined model functionality focused on BDD validation.
Only tests the essential models: CodeBERT and Sentence-Transformer.
"""

import pytest
import pytest_asyncio
from unittest.mock import Mock, patch, MagicMock
from typing import List, Dict, Any
import tempfile
import os

from src.models.model_manager import ModelManager
from src.models.huggingface_research import HuggingFaceModelResearch
from src.utils.test_helpers import TestDataGenerator
from src.config.logging import get_logger

logger = get_logger(__name__)

@pytest.mark.integration
@pytest.mark.models
class TestStreamlinedModelIntegration:
    """Integration tests for streamlined model functionality"""
    
    @pytest.fixture
    def model_manager(self):
        """Model manager instance for testing"""
        return ModelManager()
    
    @pytest.fixture
    def model_research(self):
        """Model research instance for testing"""
        return HuggingFaceModelResearch()
    
    @pytest.fixture
    def test_data_generator(self):
        """Test data generator for BDD scenarios"""
        return TestDataGenerator()
    
    def test_only_essential_models_supported(self, model_manager):
        """Test that only essential models are supported"""
        
        # Verify only essential models are in SUPPORTED_MODELS
        expected_models = ["microsoft/codebert-base", "sentence-transformers/all-MiniLM-L6-v2"]
        
        assert len(model_manager.SUPPORTED_MODELS) == 2
        for model in expected_models:
            assert model in model_manager.SUPPORTED_MODELS
        
        # Verify removed models are not supported
        removed_models = [
            "google/flan-t5-base", "microsoft/graphcodebert-base", 
            "Salesforce/codet5-base", "microsoft/unixcoder-base",
            "distilbert-base-uncased", "microsoft/codebert-base-mlm"
        ]
        for model in removed_models:
            assert model not in model_manager.SUPPORTED_MODELS
        
        logger.info("Essential models validation passed")
    
    def test_model_research_recommendations(self, model_research):
        """Test that model research only recommends essential models"""
        
        # Test requirement analysis task
        task_info = {
            "type": "requirement_analysis",
            "description": "Analyze JIRA requirements for BDD generation"
        }
        
        recommendations = model_research.recommend_models_for_task(task_info)
        
        # Should only recommend essential models
        recommended_names = [rec["model"] for rec in recommendations]
        assert "microsoft/codebert-base" in recommended_names
        assert "sentence-transformers/all-MiniLM-L6-v2" in recommended_names
        
        # Should not recommend removed models
        removed_models = ["google/flan-t5-base", "microsoft/graphcodebert-base"]
        for model in removed_models:
            assert model not in recommended_names
        
        logger.info("Model research recommendations validated")
    
    def test_bdd_validation_integration(self, model_manager, test_data_generator):
        """Test BDD validation integration with essential models"""
        
        # Generate test data
        ticket = test_data_generator.generate_jira_ticket()
        generated_bdd = {
            "feature": f"@{ticket.key} @AutoGenerated\nFeature: {ticket.summary}\n  Scenario: Test scenario",
            "step_definitions": "import { Given } from '@wdio/cucumber-framework';",
            "ticket_key": ticket.key
        }
        
        backend_result = {
            "status": "completed",
            "ticket_key": ticket.key,
            "generated_tests": generated_bdd
        }
        
        ticket_data = {
            "key": ticket.key,
            "summary": ticket.summary,
            "description": ticket.description,
            "acceptance_criteria": ticket.acceptance_criteria
        }
        
        # Mock model loading to avoid actual model downloads
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
            
            # Mock the sentence transformer encode method
            mock_model = model_manager.loaded_models["sentence-transformer"]
            mock_model.encode.return_value = [
                [0.1, 0.2, 0.3, 0.4],  # requirement embedding
                [0.12, 0.22, 0.32, 0.42]  # BDD embedding
            ]
            
            # Test BDD validation
            validation_result = model_manager.validate_bdd_against_backend_result(
                generated_bdd, backend_result, ticket_data
            )
            
            # Verify validation works
            assert "overall_score" in validation_result
            assert "requirement_coverage" in validation_result
            assert "backend_alignment" in validation_result
            assert validation_result["overall_score"] >= 0.0
            
            logger.info(f"BDD validation integration successful: {validation_result['overall_score']:.2f}")
    
    def test_model_comparison_integration(self, model_manager):
        """Test BDD comparison functionality"""
        
        generated_bdd = {
            "feature": "Feature: User login\n  Scenario: Login success",
            "step_definitions": "Given('user is on login page', () => {});"
        }
        
        expected_bdd = {
            "feature": "Feature: User authentication\n  Scenario: Successful login",
            "step_definitions": "Given('user navigates to login', () => {});"
        }
        
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
            
            # Mock similarity calculation
            mock_model = model_manager.loaded_models["sentence-transformer"]
            mock_model.encode.return_value = [
                [0.1, 0.2, 0.3, 0.4],  # Generated feature
                [0.15, 0.25, 0.35, 0.45],  # Expected feature  
                [0.2, 0.3, 0.4, 0.5],  # Generated steps
                [0.25, 0.35, 0.45, 0.55]  # Expected steps
            ]
            
            comparison_result = model_manager.compare_bdd_with_expected(generated_bdd, expected_bdd)
            
            assert "feature_similarity" in comparison_result
            assert "step_similarity" in comparison_result
            assert "overall_similarity" in comparison_result
            assert comparison_result["overall_similarity"] >= 0.0
            
            logger.info(f"BDD comparison integration successful: {comparison_result['overall_similarity']:.2f}")
    
    def test_model_loading_performance(self, model_manager):
        """Test model loading performance with essential models only"""
        
        import time
        
        # Test loading each essential model
        essential_models = ["microsoft/codebert-base", "sentence-transformers/all-MiniLM-L6-v2"]
        
        for model_name in essential_models:
            start_time = time.time()
            
            # Mock loading to test interface without actual download
            with patch.object(model_manager, '_load_transformers_model') as mock_load, \
                 patch.object(model_manager, '_load_sentence_transformer') as mock_sentence:
                
                mock_load.return_value = Mock()
                mock_sentence.return_value = Mock()
                
                result = model_manager.load_model(model_name)
                load_time = time.time() - start_time
                
                assert result["success"] is True
                assert load_time < 1.0  # Should be fast with mocking
                
                logger.info(f"Model {model_name} loading test completed in {load_time:.3f}s")
    
    def test_error_handling_with_unsupported_models(self, model_manager):
        """Test error handling when trying to load unsupported models"""
        
        unsupported_models = [
            "google/flan-t5-base",
            "microsoft/graphcodebert-base", 
            "Salesforce/codet5-base",
            "nonexistent/model"
        ]
        
        for model_name in unsupported_models:
            result = model_manager.load_model(model_name)
            assert result["success"] is False
            assert "error" in result
            assert "not supported" in result["error"].lower() or "not found" in result["error"].lower()
        
        logger.info("Unsupported model error handling validated")
    
    def test_memory_efficiency_with_streamlined_models(self, model_manager):
        """Test memory efficiency with only essential models loaded"""
        
        # Mock memory usage tracking
        initial_memory = 100  # MB baseline
        
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {
                 "microsoft/codebert-base": Mock(),
                 "sentence-transformer": Mock()
             }):
            
            # Simulate memory usage with only 2 models vs many models
            current_memory = initial_memory + (len(model_manager.loaded_models) * 50)  # 50MB per model
            
            # With only 2 models, should be much more efficient than 10+ models
            assert current_memory <= 300  # 100 base + 2*50 + overhead
            assert len(model_manager.loaded_models) == 2
            
            logger.info(f"Memory efficiency test passed: {current_memory}MB with {len(model_manager.loaded_models)} models")


@pytest.mark.integration
@pytest.mark.performance
class TestStreamlinedModelPerformance:
    """Performance tests for streamlined model operations"""
    
    def test_validation_performance_with_essential_models(self, model_manager):
        """Test validation performance with only essential models"""
        
        # Large BDD scenario to test performance
        large_bdd = {
            "feature": "Feature: Complex application\n" + "\n".join([
                f"  Scenario: Test scenario {i}" for i in range(10)
            ]),
            "step_definitions": "import test\n" + "\n".join([
                f"Given('step {i}', () => {{}});" for i in range(20)
            ]),
            "ticket_key": "PERF-001"
        }
        
        backend_result = {
            "status": "completed",
            "ticket_key": "PERF-001",
            "generated_tests": large_bdd
        }
        
        ticket_data = {
            "key": "PERF-001",
            "summary": "Performance test",
            "description": "Large scenario for performance testing"
        }
        
        import time
        start_time = time.time()
        
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
            
            # Mock fast embedding calculation
            mock_model = model_manager.loaded_models["sentence-transformer"]
            mock_model.encode.return_value = [[0.1, 0.2], [0.1, 0.2]]
            
            validation_result = model_manager.validate_bdd_against_backend_result(
                large_bdd, backend_result, ticket_data
            )
            
            execution_time = time.time() - start_time
            
            # Should be fast with streamlined models
            assert execution_time < 0.5
            assert validation_result["overall_score"] >= 0.0
            
            logger.info(f"Validation performance: {execution_time:.3f}s with streamlined models")
    
    def test_concurrent_validation_performance(self, model_manager):
        """Test concurrent validation performance"""
        
        import asyncio
        import time
        
        async def validate_single_bdd(bdd_id: int):
            """Validate a single BDD scenario"""
            
            bdd = {
                "feature": f"Feature: Test {bdd_id}\n  Scenario: Scenario {bdd_id}",
                "step_definitions": f"Given('step {bdd_id}', () => {{}});",
                "ticket_key": f"TEST-{bdd_id}"
            }
            
            backend_result = {"status": "completed", "ticket_key": f"TEST-{bdd_id}"}
            ticket_data = {"key": f"TEST-{bdd_id}", "summary": f"Test {bdd_id}"}
            
            with patch.object(model_manager, '_is_model_loaded', return_value=True), \
                 patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
                
                mock_model = model_manager.loaded_models["sentence-transformer"]
                mock_model.encode.return_value = [[0.1, 0.2], [0.1, 0.2]]
                
                return model_manager.validate_bdd_against_backend_result(bdd, backend_result, ticket_data)
        
        async def run_concurrent_validations():
            """Run multiple validations concurrently"""
            tasks = [validate_single_bdd(i) for i in range(5)]
            return await asyncio.gather(*tasks)
        
        start_time = time.time()
        
        # Run the async function in sync context
        results = asyncio.run(run_concurrent_validations())
        
        execution_time = time.time() - start_time
        
        # All validations should complete
        assert len(results) == 5
        for result in results:
            assert result["overall_score"] >= 0.0
        
        # Should be efficient with streamlined models
        assert execution_time < 2.0
        
        logger.info(f"Concurrent validation performance: {execution_time:.3f}s for 5 validations")


@pytest.mark.integration
@pytest.mark.regression
class TestStreamlinedModelRegression:
    """Regression tests to ensure streamlined models maintain functionality"""
    
    def test_backward_compatibility_validation_interface(self, model_manager):
        """Test that validation interface remains backward compatible"""
        
        # Test data that should work with old and new interface
        bdd = {
            "feature": "Feature: Test\n  Scenario: Test scenario",
            "step_definitions": "Given('test', () => {});",
            "ticket_key": "COMPAT-001"
        }
        
        backend_result = {"status": "completed", "ticket_key": "COMPAT-001"}
        ticket_data = {"key": "COMPAT-001", "summary": "Compatibility test"}
        
        with patch.object(model_manager, '_is_model_loaded', return_value=True), \
             patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
            
            mock_model = model_manager.loaded_models["sentence-transformer"]
            mock_model.encode.return_value = [[0.1, 0.2], [0.1, 0.2]]
            
            # This should work exactly as before
            result = model_manager.validate_bdd_against_backend_result(bdd, backend_result, ticket_data)
            
            # Verify all expected fields exist
            expected_fields = ["overall_score", "requirement_coverage", "scenario_completeness", 
                             "step_accuracy", "backend_alignment", "recommendations"]
            for field in expected_fields:
                assert field in result
            
            logger.info("Backward compatibility validation passed")
    
    def test_model_research_consistency(self, model_research):
        """Test that model research remains consistent after streamlining"""
        
        # Test different task types
        tasks = [
            {"type": "requirement_analysis", "description": "Analyze requirements"},
            {"type": "bdd_validation", "description": "Validate BDD scenarios"},
            {"type": "code_understanding", "description": "Understand code structure"}
        ]
        
        for task in tasks:
            recommendations = model_research.recommend_models_for_task(task)
            
            # Should always return recommendations
            assert len(recommendations) > 0
            
            # Should only recommend supported models
            for rec in recommendations:
                assert rec["model"] in ["microsoft/codebert-base", "sentence-transformers/all-MiniLM-L6-v2"]
                assert "score" in rec
                assert "reasoning" in rec
        
        logger.info("Model research consistency validated")
    
    def test_validation_scoring_consistency(self, model_manager):
        """Test that validation scoring remains consistent"""
        
        # Test cases with known expected score ranges
        test_cases = [
            {
                "name": "high_quality_bdd",
                "bdd": {
                    "feature": """@TEST-001 @AutoGenerated
Feature: User authentication
  As a user
  I want to log into the application
  So that I can access my account

  Scenario: Successful login
    Given user is on login page
    When user enters valid credentials
    Then user should be redirected to dashboard
""",
                    "step_definitions": """import { Given, When, Then } from '@wdio/cucumber-framework';
Given('user is on login page', async () => {
    await browser.url('/login');
});""",
                    "ticket_key": "TEST-001"
                },
                "expected_score_range": (0.6, 1.0)
            },
            {
                "name": "poor_quality_bdd", 
                "bdd": {
                    "feature": "Feature: Incomplete",
                    "step_definitions": "// TODO",
                    "ticket_key": "TEST-002"
                },
                "expected_score_range": (0.0, 0.4)
            }
        ]
        
        for test_case in test_cases:
            backend_result = {"status": "completed", "ticket_key": test_case["bdd"]["ticket_key"]}
            ticket_data = {"key": test_case["bdd"]["ticket_key"], "summary": "Test"}
            
            with patch.object(model_manager, '_is_model_loaded', return_value=True), \
                 patch.object(model_manager, 'loaded_models', {"sentence-transformer": Mock()}):
                
                # Mock appropriate similarity for the test case
                if "high_quality" in test_case["name"]:
                    mock_similarity = [[0.1, 0.2], [0.12, 0.22]]  # High similarity
                else:
                    mock_similarity = [[0.1, 0.2], [0.9, 0.8]]   # Low similarity
                
                mock_model = model_manager.loaded_models["sentence-transformer"]
                mock_model.encode.return_value = mock_similarity
                
                result = model_manager.validate_bdd_against_backend_result(
                    test_case["bdd"], backend_result, ticket_data
                )
                
                min_score, max_score = test_case["expected_score_range"]
                actual_score = result["overall_score"]
                
                assert min_score <= actual_score <= max_score, \
                    f"{test_case['name']}: Score {actual_score} not in range [{min_score}, {max_score}]"
        
        logger.info("Validation scoring consistency verified")